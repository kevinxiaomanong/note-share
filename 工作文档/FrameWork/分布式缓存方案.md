## Trocks

**资源地址**

http://conf.ctripcorp.com/pages/viewpage.action?pageId=487981202（Trocks在携程的设计与落地）



优势：

结合了内存存储和持久化存储两方面的优点，即拥有接近内存型存储的读写性能，又有和持久化存储等同的数据完整型和可靠性，在面对高速读写时对服务器磁盘的压力小于传统关系型数据库



**持久化、高速、海量、易用**



当然这种KV系统有个普遍问题，即在表现复杂关系时会显得有些啰嗦和重复，这方面不如Sql语言有表现力，但这个问题其实完全可以在应用层面通过封装来解决



可以直接通过CRedis来访问Trocks的客户端



**锁操作**

在使用DB或Cache的时候其实不可避免读写冲突，Trocks提供锁操作来保障多线程操作

使用时注意：

1. 加锁和解锁配套使用，单独解锁会报错，如果加锁后不解锁1分钟后自动解除锁定
2. 锁命令都有client_tid和sequence需要传入，例如getl client_tid sequence key，其中client_tid是用于访问隔离，建议一个客户端连接使用一个client_tid，sequence是一个client_tid下数据执行的序号，要求始终增加可以不连续，要求解锁命令的client_tid和sequence要和加锁命令一致



**高可用处理**

Trocks可通过主从关系构建集群，并将数据从Master同步到Slave系统



**使用场景**

1、缓存 

2、秒杀 会出现集中访问持久化资源的情况 给持久化资源带来巨大的访问压力 很多方案会考虑通过缓存来削减持久化资源的访问次数，来减轻系统压力，但这带来了副本一致性的问题，直接使用Trocks（锁机制）就不需要考虑

3、超大基础数据存储（用户画像），一些数据，如价格、库存、操作日志等，这类数据的特征是结构简单，但是数据量极大而且可能存在大量读写，如果用Redis这样的内存KV会占用大量内存空间，成本上升，并且可能存在冷热不均的情况，用Trocks存储就无需考虑冷热数据问题，并且空间和持久化上都有保证



## Redis集群原理

Redis集群采用主从这种架构，为什么不采用负载均衡类呢？

是因为Redis的场景大多是读多写少的，所以我们会做主从，做了主从之后就可以实现读写分离

搭建过程：只需在执行一个slaveof命令即可



**数据同步**

那么主从节点之间是一定要进行数据同步的来保证数据的不一致性

- 主从第一次同步是全量同步 

master节点执行bgsave，后台生成RDB，并在RDB期间的命令写入repl_baklog的缓冲区

最终slave会加载RDB文件、同时执行缓冲区repl_baklog内的命令

后期有独立进程不断从master的缓冲区去取指令执行   



master如何判断slave是否第一次同步数据

- Replication Id，即数据集标记，id一致说明是同一数据集，每个master有唯一的replid，slave会继承master的replid
- offset 随着记录在内存缓冲区数据增多而增大，slave同步时会记录当前同步的offset，如果说slave的offset小于master的offset，说明slave数据落后于master，需要更新

所以slave做数据同步必须要带上自己的replic ID数据集id 和 Offset



**增量同步**：

如果slave重启后同步，会将repl_baklog内存缓冲区中offset后的数据执行

 repl_baklog是一个环形数组，只要不覆写掉了slave保存的offset进度都是可以同步的，如果被覆写了那只能再次进行全量同步

 

**优化**

-  在master中配置repl-diskless-sync yes，即启用无磁盘复制，原理是把IO不写到磁盘而是直接写到网络，减少一次磁盘的读写带来的IO消耗
- 让Redis单节点上的内存占用不要太大，减少RDB导致过多的磁盘IO
- 适当提高repl_baklog的大小，同时发现slave宕机时尽快进行故障恢复，尽可能避免全量同步
- 采用主-从-从链式结构，减少master压力



**master节点宕机** 

Redis提供了哨兵sentinel机制实现主从集群的自动故障恢复





**哨兵**

- 监控 Sentinel会不断检查master和slave是否按预期工作
-  如果发生master故障，Sentinel会将一个slave提升为master
-  Sentinel会作为Redis客户端的服务发现来源， 当集群发生故障转移时，会将最新消息推送给Redis客户端

Redis Sentinel本身也是一个分布式系统，一般我们会让Sentinel进程协同合作



**服务状态监控**

Sentinel基于心跳机制检测服务状态，每隔1s向集群的每个实例发送ping命令

- 如果某sentinel发现某实例未在规定时间响应则认为该实例主观下线
- 如果指定数量quorum的sentinel都认为该实例主观下线则该实例客观下线，quorum最高超过实例数量一半



**选举新的master**

一旦发现master故障，sentinel需要再slave中选择一个作为新的master

- 首先判断slave与master断开时间长短，如果超过指定值则排除该slave（过旧）
- 判断slave-priority，越小优先级越高，为0不参加选举
- 如果slave-priority一样，则offset越大则优先级越高
- 最后判断slave节点的运行id大小，越小优先级越高



**故障转移**

- sentinel给备选的节点发送slaveof on one命令，让该节点成为master
- 广播让其余slave 执行新的slaveof命令，并将故障节点进行强制标记 



**Sentinel应用**

可以看到引入Sentinel之后，我们要做的事情就是引依赖、配置哨兵的地址、配读写分离

主从切换、路由都交给Sentinel来做



**分片集群**

前面的主从和哨兵解决高可用、高并发读的问题，但还有两个问题没有解决

- 海量数据存储
- 高并发写

我们可以使用分片集群来解决

- 集群中有多个master，每个master保存不同数据
- 每个master都可以有多个slave节点 
- master之间通过ping检测彼此健康状态
- 客户端请求可以访问集群任意节点，最终都会被转发到正确节点



**散列插槽**

Redis会把每一个master节点映射到0-16383共16384个Hash slot插槽上

数据key不是和节点绑定，而是与插槽绑定，redis根据key的有效部分通过CRC16算法得到一个hash值然后对16384取余，然后得到slot值

因为Redis集群节点会存在宕机、伸缩实例的情况，插槽的存在可以使得数据和节点进行解耦，当节点变更时我们将对应插槽进行转移。



有时候我们希望某一类数据都存储到同一个Redis实例上，这样当我们去查询某一类时就不需要重定向从单个节点上查数据即可,实现方法：

可以让同一类数据的key都已{typeId}为前缀，保证key的有效部分相同即可



**集群伸缩**

即这个集群可以根据实际生产需要动态改变实例数量 

















